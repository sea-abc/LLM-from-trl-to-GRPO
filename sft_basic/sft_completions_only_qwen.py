"""
这个代码实现了：
1. 基于Qwen3-4B-Instruct模型的LoRA指令微调
2. 手动的损失掩码逻辑，确保只计算生成回答的损失
3. 使用pytorch的传统深度学习预训练框架
"""

# Qwen3-4B-Instruct 模型
# 基于 sft_simple_qwen.py 的可运行结构，结合损失掩码逻辑

# ====================== 1. 国内镜像加速（必须放在库导入前面） ======================
import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

# ====================== 2. 导入库 ======================
import torch
import gc
import functools
from transformers import AutoModelForCausalLM, AutoTokenizer 
from peft import LoraConfig, TaskType, get_peft_model

# ====================== 3. 模型和分词器 ======================
# 清理显存和缓存
torch.cuda.empty_cache()
gc.collect()


model_name = "Qwen/Qwen3-4B-Instruct-2507"

tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    trust_remote_code=True,
    padding_side="right"
)

# 设置pad_token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.eos_token_id


# 加载模型
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    dtype=torch.bfloat16,
    trust_remote_code=True
).to("cuda")

# ====================== 4. 配置LoRA ======================
# 使用LoRA代替参数冻结，实现高效微调
peft_config = LoraConfig(
    r=8,
    target_modules=["self_attn.q_proj", "self_attn.k_proj", "self_attn.v_proj", 
                   "self_attn.o_proj", "mlp.gate_proj", "mlp.up_proj", "mlp.down_proj"],
    task_type=TaskType.CAUSAL_LM,
    lora_alpha=16,
    lora_dropout=0.05
)

# 转换为PeftModel并冻结非LoRA参数
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)

# ====================== 5. 自定义Dataset和Collate函数 ======================
class SFTDataset:
    """
    简单的SFT数据集类
    作用就是给原来的对话列表添加开头结尾的模板
    """
    
    def __init__(self, dialogs, tokenizer):
        """
        dialogs: 对话列表，每个对话是包含role和content的字典列表
        tokenizer: 分词器
        """
        self.dialogs = dialogs
        self.tokenizer = tokenizer
    
    def __getitem__(self, index):
        """返回应用聊天模板后的文本"""
        dialog = self.dialogs[index]
        # 应用聊天模板但不添加生成提示
        chat_text = self.tokenizer.apply_chat_template(
            dialog, 
            tokenize=False,  # 就是不变成数字
            add_generation_prompt=False # 不添加生成提示，因为我们只训练回答部分
            )
        return chat_text
    
    def __len__(self):
        return len(self.dialogs)
"""
# 原始输入
dialog = [
    {"role": "user", "content": "用优美的词来夸一夸我吧"},
    {"role": "assistant", "content": "您如春日暖阳般温暖，如夏日清风般怡人"}
]

# 应用聊天模板后的输出对比
# 假设使用 Qwen3-4B-Instruct 模型

# 参数设置对比表格：
# 
# | 参数设置 (add_generation_prompt) | 输出文本 (chat_text) | 用途说明 |
# |-----------------------------------|----------------------|----------|
# | False                             | <|im_start|>user\n用优美的词来夸一夸我吧<|im_end|>\n<|im_start|>assistant\n您如春日暖阳般温暖，如夏日清风般怡人<|im_end|> | 用于训练：包含完整对话，没有后续生成提示 |
# | True                              | <|im_start|>user\n用优美的词来夸一夸我吧<|im_end|>\n<|im_start|>assistant\n您如春日暖阳般温暖，如夏日清风般怡人<|im_end|>\n<|im_start|>assistant | 用于推理：在完整对话后添加助手标记，提示模型开始生成 |

# 具体解释：
# 1. 当 add_generation_prompt=False 时，输出是完整的对话历史，用于训练模型学习已有的问答对
# 2. 当 add_generation_prompt=True 时，输出在完整对话历史末尾添加了 <|im_start|>assistant 标记，
#    提示模型"现在该你回答了"，用于实际对话生成
"""



def sft_collate(batch, tokenizer, max_length=500):
    """
    数据整理函数 - 创建损失掩码，只对助手回答部分计算损失
    
    参数说明：
    - batch: 文本列表，每个元素是一个完整的对话（包含system、user、assistant部分）
    - tokenizer: 分词器，用于将文本转换为模型可理解的数字
    - max_length: 最大序列长度，超过的部分会被截断
    
    返回值：
    - inputs: 包含tokenized后的输入数据（input_ids、attention_mask等）
    - loss_mask: 损失掩码，0表示不计算损失，1表示计算损失
    
    工作原理：
    1. 将批量文本转换为数字序列
    2. 找到每个对话中助手开始回答的位置
    3. 创建掩码，只让助手回答部分的损失被计算
    4. 将数据转移到GPU上
    
    示例：
    输入文本："<|im_start|>system\nYou are a helpful assistant.<|im_end|>
    <|im_start|>user\n什么是LoRA？<|im_end|>
    <|im_start|>assistant\nLoRA是一种参数高效微调方法。<|im_end|>"
    
    掩码结果：
    [0, 0, 0, ..., 0, 1, 1, 1, ..., 1]  # 前面的0对应system和user部分，后面的1对应assistant回答部分
    """
    
    # 1. 分词处理：将文本转换为数字序列，并进行padding和truncation
    # padding=True：短文本会在末尾填充0，使所有样本长度一致
    # truncation=True：超过max_length的文本会被截断
    # return_tensors="pt"：返回PyTorch张量格式
    inputs = tokenizer(batch, max_length=max_length, padding=True, truncation=True, return_tensors="pt")
    
    # 2. 识别assistant回答的起始位置
    # Qwen3模型使用特殊标记来区分不同角色的内容
    # 助手开始的标记是：<|im_start|>assistant
    assistant_start_str = "<|im_start|>assistant"
    # 将标记转换为数字序列，add_special_tokens=False表示不添加额外的特殊标记
    assistant_start_ids = tokenizer(assistant_start_str, add_special_tokens=False)['input_ids']
    assistant_start_len = len(assistant_start_ids)
    
    # 3. 为每个样本创建损失掩码
    loss_mask = []  # 存储所有样本的损失掩码
    input_ids = inputs['input_ids']  # 获取tokenized后的输入数据
    
    # 遍历每个样本
    for i, input_id in enumerate(input_ids):
        # 初始化掩码：默认所有位置都不计算损失（用0表示） 
        # 例：input_id = [100, 200, 300, 400, 500]
        mask = [0] * len(input_id) # 例：mask = [0, 0, 0, 0, 0]
        
        # 将输入数据转换为列表，方便后续操作
        input_id_list = input_id.tolist()
        
        # 查找assistant标记在当前样本中的位置
        # 从前往后搜索整个序列
        for j in range(len(input_id_list) - assistant_start_len + 1):
            # 检查当前位置是否匹配assistant标记
            if input_id_list[j:j+assistant_start_len] == assistant_start_ids:
                # 找到标记后，确定助手回答的实际起始位置
                # Qwen模板格式：标记后面会有一个换行符（\n），所以需要+1跳过换行
                assistant_content_start = j + assistant_start_len + 1
                
                # 将助手回答部分的掩码设置为1，表示计算损失
                # 从回答起始位置到序列末尾都设置为1
                mask[assistant_content_start:] = [1] * (len(input_id_list) - assistant_content_start)
                
                # 找到一个标记后就可以退出循环了（每个对话只有一个assistant部分）
                break
        
        # 将当前样本的掩码添加到列表中
        loss_mask.append(mask)
    
    # 4. 将数据转移到GPU上，加速后续计算
    # inputs是包含tokenized后数据的字典（如input_ids、attention_mask等）
    # 使用字典推导式将字典中所有tensor值转移到GPU设备
    # 必须将数据和模型放在同一设备上才能进行计算
    inputs = {k: v.to("cuda") for k, v in inputs.items()}
    # 将损失掩码也转移到GPU上
    loss_mask = torch.tensor(loss_mask, device="cuda")
    
    # 返回处理后的输入数据和损失掩码
    return inputs, loss_mask

# ====================== 6. 训练数据 ======================
# 示例训练数据
dialogs = [
    [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "什么是LoRA？"},
        {"role": "assistant", "content": "LoRA是一种参数高效微调方法，通过在模型层中插入低秩矩阵来减少训练参数。"}
    ],
    [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "用优美的词来夸一夸我吧"},
        {"role": "assistant", "content": "您如春日暖阳般温暖，如夏日清风般怡人，如秋日明月般清朗，如冬日初雪般纯净。您的眼眸中蕴含着星辰大海，您的微笑里传递着无尽善意。"}
    ],
    [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Python有哪些优势？"},
        {"role": "assistant", "content": "Python具有简洁易读的语法、丰富的第三方库、跨平台兼容性、强大的社区支持、适合快速开发等优势。"}
    ]
]

# 创建数据集和数据加载器
dataset = SFTDataset(dialogs, tokenizer)

# 创建partial collate函数
# 使用functools.partial将tokenizer和max_length参数绑定到sft_collate函数
# 这样data_loader在调用collate_fn时就不需要再传递这些参数了
collate_fn = functools.partial(
    sft_collate,          # 要绑定的原始函数
    tokenizer=tokenizer,  # 固定tokenizer参数
    max_length=500        # 固定max_length参数为500
)

data_loader = torch.utils.data.DataLoader(
    dataset,
    batch_size=1,
    collate_fn=collate_fn,
    shuffle=True
)

# ====================== 7. 训练 ======================
epochs = 3

for epoch in range(epochs):
    for step, (inputs, loss_mask) in enumerate(data_loader):
        # 前向传播
        outputs = model(**inputs)
        logits = outputs.logits  # 形状: [batch_size, sequence_length, vocab_size]，例如 [1, 100, 151936]
        
        # 计算损失，只对回答部分计算
        # 将logits向右偏移一位（移除最后一个token）
        shift_logits = logits[:, :-1, :].contiguous()  # 形状: [batch_size, sequence_length-1, vocab_size]，例如 [1, 99, 151936]
        # 将labels向左偏移一位（移除第一个token）
        shift_labels = inputs["input_ids"][:, 1:].contiguous()  # 形状: [batch_size, sequence_length-1]，例如 [1, 99]
        # 将损失掩码向左偏移一位（与labels对齐）
        shift_loss_mask = loss_mask[:, 1:].contiguous()  # 形状: [batch_size, sequence_length-1]，例如 [1, 99]
        
        # 计算交叉熵损失
        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id, reduction="none")
        # 将logits和labels展平为二维和一维，以便计算交叉熵
        # shift_logits.view(-1, vocab_size) → 形状: [batch_size*(sequence_length-1), vocab_size]，例如 [99, 151936]
        # shift_labels.view(-1) → 形状: [batch_size*(sequence_length-1)]，例如 [99]
        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))  
        # 形状: [batch_size*(sequence_length-1)]，例如 [99]
        
        # 应用损失掩码
        # shift_loss_mask.view(-1) → 形状: [batch_size*(sequence_length-1)]，例如 [99]
        loss = loss * shift_loss_mask.view(-1)  # 形状不变: [99]，只保留回答部分的损失
        # 计算平均损失：总和除以掩码中1的数量（避免除以0）
        loss = loss.sum() / (shift_loss_mask.sum() + 1e-8)  # 形状: 标量，例如 2.7778
        
        print(f"Epoch: {epoch+1}, Step: {step+1}, Loss: {loss.item()}")
        
        # 反向传播和优化
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

# ====================== 8. 推理测试 ======================

print("\n推理测试：")
test_dialog = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "请用最优美的话语夸赞和鼓励我："}
]

test_input = tokenizer.apply_chat_template(test_dialog, add_generation_prompt=True, return_tensors="pt").to("cuda")

with torch.no_grad():
    test_output = model.generate(
        test_input,
        max_new_tokens=200,
        temperature=0.7,
        attention_mask=torch.ones_like(test_input)
    )

# 直接解码输出并提取助手回答部分
answer = tokenizer.decode(test_output[0][test_input.shape[-1]:], skip_special_tokens=True)
print(f"user: {test_dialog[-1]['content']}")
print(f"assistant: {answer}")

# 清理显存
torch.cuda.empty_cache()
gc.collect()

# ====================== 9. 保存模型 ======================
# 保存LoRA模型
model.save_pretrained("./qwen3_lora_completions_only")
tokenizer.save_pretrained("./qwen3_lora_completions_only")
print("\nLoRA模型已保存到 ./qwen3_lora_completions_only")
