class trl.ScriptArguments
( dataset_name: str | None = Nonedataset_config: str | None = Nonedataset_train_split: str = 'train'dataset_test_split: str = 'test'dataset_streaming: bool = Falsegradient_checkpointing_use_reentrant: bool = Falseignore_bias_buffers: bool = False )



class ModelConfig:
    """
    Configuration class for the models.

    Using [`~transformers.HfArgumentParser`] we can turn this class into
    [argparse](https://docs.python.org/3/library/argparse#module-argparse) arguments that can be specified on the
    command line.

    Parameters:
        model_name_or_path (`str`, *optional*):
            Model checkpoint for weights initialization.
        model_revision (`str`, *optional*, defaults to `"main"`):
            Specific model version to use. It can be a branch name, a tag name, or a commit id.
        dtype (`Literal["auto", "bfloat16", "float16", "float32"]`, *optional*):
            Override the default `torch.dtype` and load the model under this dtype. Possible values are

                - `"bfloat16"`: `torch.bfloat16`
                - `"float16"`: `torch.float16`
                - `"float32"`: `torch.float32`
                - `"auto"`: Automatically derive the dtype from the model's weights.

        trust_remote_code (`bool`, *optional*, defaults to `False`):
            Whether to allow for custom models defined on the Hub in their own modeling files. This option should only
            be set to `True` for repositories you trust and in which you have read the code, as it will execute code
            present on the Hub on your local machine.
        attn_implementation (`str`, *optional*):
            Which attention implementation to use. More information in the [Kernels Hub Integrations
            Guide](kernels_hub).
        use_peft (`bool`, *optional*, defaults to `False`):
            Whether to use PEFT for training.
        lora_r (`int`, *optional*, defaults to `16`):
            LoRA R value.
        lora_alpha (`int`, *optional*, defaults to `32`):
            LoRA alpha.
        lora_dropout (`float`, *optional*, defaults to `0.05`):
            LoRA dropout.
        lora_target_modules (`str | list[str]`, *optional*):
            LoRA target modules.
        lora_target_parameters (`str | list[str]`, *optional*):
            List of target parameters for LoRA.
        lora_modules_to_save (`list[str]`, *optional*):
            Model layers to unfreeze & train.
        lora_task_type (`str`, *optional*, defaults to `"CAUSAL_LM"`):
            Task type to pass for LoRA (use `"SEQ_CLS"` for reward modeling).
        use_rslora (`bool`, *optional*, defaults to `False`):
            Whether to use Rank-Stabilized LoRA, which sets the adapter scaling factor to `lora_alpha/âˆšr`, instead of
            the original default value of `lora_alpha/r`.
        use_dora (`bool`, *optional*, defaults to `False`):
            Enable [Weight-Decomposed Low-Rank Adaptation (DoRA)](https://huggingface.co/papers/2402.09353). This
            technique decomposes the updates of the weights into two parts, magnitude and direction. Direction is
            handled by normal LoRA, whereas the magnitude is handled by a separate learnable parameter. This can
            improve the performance of LoRA, especially at low ranks. Right now, DoRA only supports linear and Conv2D
            layers. DoRA introduces a bigger overhead than pure LoRA, so it is recommended to merge weights for
            inference.
        load_in_8bit (`bool`, *optional*, defaults to `False`):
            Whether to use 8 bit precision for the base model. Works only with LoRA.
        load_in_4bit (`bool`, *optional*, defaults to `False`):
            Whether to use 4 bit precision for the base model. Works only with LoRA.
        bnb_4bit_quant_type (`str`, *optional*, defaults to `"nf4"`):
            Quantization type (`"fp4"` or `"nf4"`).
        use_bnb_nested_quant (`bool`, *optional*, defaults to `False`):
            Whether to use nested quantization.
    """

class trl.PPOConfig
( output_dir: typing.Optional[str] = Noneoverwrite_output_dir: bool = Falsedo_train: bool = Falsedo_eval: bool = Falsedo_predict: bool = Falseeval_strategy: typing.Union[transformers.trainer_utils.IntervalStrategy, str] = 'no'prediction_loss_only: bool = Falseper_device_train_batch_size: int = 8per_device_eval_batch_size: int = 8per_gpu_train_batch_size: typing.Optional[int] = Noneper_gpu_eval_batch_size: typing.Optional[int] = Nonegradient_accumulation_steps: int = 1eval_accumulation_steps: typing.Optional[int] = Noneeval_delay: float = 0torch_empty_cache_steps: typing.Optional[int] = Nonelearning_rate: float = 5e-05weight_decay: float = 0.0adam_beta1: float = 0.9adam_beta2: float = 0.999adam_epsilon: float = 1e-08max_grad_norm: float = 1.0num_train_epochs: float = 3.0max_steps: int = -1lr_scheduler_type: typing.Union[transformers.trainer_utils.SchedulerType, str] = 'linear'lr_scheduler_kwargs: dict | str | None = Nonewarmup_ratio: float = 0.0warmup_steps: int = 0log_level: str = 'passive'log_level_replica: str = 'warning'log_on_each_node: bool = Truelogging_dir: typing.Optional[str] = Nonelogging_strategy: typing.Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps'logging_first_step: bool = Falselogging_steps: float = 10logging_nan_inf_filter: bool = Truesave_strategy: typing.Union[transformers.trainer_utils.SaveStrategy, str] = 'steps'save_steps: float = 500save_total_limit: typing.Optional[int] = Nonesave_safetensors: bool = Truesave_on_each_node: bool = Falsesave_only_model: bool = Falserestore_callback_states_from_checkpoint: bool = Falseno_cuda: bool = Falseuse_cpu: bool = Falseuse_mps_device: bool = Falseseed: int = 42data_seed: typing.Optional[int] = Nonejit_mode_eval: bool = Falsebf16: bool | None = Nonefp16: bool = Falsefp16_opt_level: str = 'O1'half_precision_backend: str = 'auto'bf16_full_eval: bool = Falsefp16_full_eval: bool = Falsetf32: typing.Optional[bool] = Nonelocal_rank: int = -1ddp_backend: typing.Optional[str] = Nonetpu_num_cores: typing.Optional[int] = Nonetpu_metrics_debug: bool = Falsedebug: typing.Union[str, list[transformers.debug_utils.DebugOption]] = ''dataloader_drop_last: bool = Falseeval_steps: typing.Optional[float] = Nonedataloader_num_workers: int = 0dataloader_prefetch_factor: typing.Optional[int] = Nonepast_index: int = -1run_name: str | None = Nonedisable_tqdm: typing.Optional[bool] = Noneremove_unused_columns: bool = Truelabel_names: typing.Optional[list[str]] = Noneload_best_model_at_end: bool = Falsemetric_for_best_model: typing.Optional[str] = Nonegreater_is_better: typing.Optional[bool] = Noneignore_data_skip: bool = Falsefsdp: typing.Union[list[transformers.trainer_utils.FSDPOption], str, NoneType] = Nonefsdp_min_num_params: int = 0fsdp_config: typing.Union[dict[str, typing.Any], str, NoneType] = Nonefsdp_transformer_layer_cls_to_wrap: typing.Optional[str] = Noneaccelerator_config: typing.Union[dict, str, NoneType] = Noneparallelism_config: typing.Optional[accelerate.parallelism_config.ParallelismConfig] = Nonedeepspeed: typing.Union[dict, str, NoneType] = Nonelabel_smoothing_factor: float = 0.0optim: typing.Union[transformers.training_args.OptimizerNames, str] = 'adamw_torch_fused'optim_args: typing.Optional[str] = Noneadafactor: bool = Falsegroup_by_length: bool = Falselength_column_name: str = 'length'report_to: typing.Union[NoneType, str, list[str]] = Noneproject: str = 'huggingface'trackio_space_id: typing.Optional[str] = 'trackio'ddp_find_unused_parameters: typing.Optional[bool] = Noneddp_bucket_cap_mb: typing.Optional[int] = Noneddp_broadcast_buffers: typing.Optional[bool] = Nonedataloader_pin_memory: bool = Truedataloader_persistent_workers: bool = Falseskip_memory_metrics: bool = Trueuse_legacy_prediction_loop: bool = Falsepush_to_hub: bool = Falseresume_from_checkpoint: typing.Optional[str] = Nonehub_model_id: typing.Optional[str] = Nonehub_strategy: typing.Union[transformers.trainer_utils.HubStrategy, str] = 'every_save'hub_token: typing.Optional[str] = Nonehub_private_repo: typing.Optional[bool] = Nonehub_always_push: bool = Falsehub_revision: typing.Optional[str] = Nonegradient_checkpointing: bool = Truegradient_checkpointing_kwargs: typing.Union[dict[str, typing.Any], str, NoneType] = Noneinclude_inputs_for_metrics: bool = Falseinclude_for_metrics: list = <factory>eval_do_concat_batches: bool = Truefp16_backend: str = 'auto'push_to_hub_model_id: typing.Optional[str] = Nonepush_to_hub_organization: typing.Optional[str] = Nonepush_to_hub_token: typing.Optional[str] = Nonemp_parameters: str = ''auto_find_batch_size: bool = Falsefull_determinism: bool = Falsetorchdynamo: typing.Optional[str] = Noneray_scope: typing.Optional[str] = 'last'ddp_timeout: int = 1800torch_compile: bool = Falsetorch_compile_backend: typing.Optional[str] = Nonetorch_compile_mode: typing.Optional[str] = Noneinclude_tokens_per_second: bool = Falseinclude_num_input_tokens_seen: typing.Union[str, bool] = Falseneftune_noise_alpha: typing.Optional[float] = Noneoptim_target_modules: typing.Union[NoneType, str, list[str]] = Nonebatch_eval_metrics: bool = Falseeval_on_start: bool = Falseuse_liger_kernel: bool = Falseliger_kernel_config: typing.Optional[dict[str, bool]] = Noneeval_use_gather_object: bool = Falseaverage_tokens_across_devices: bool = Truedataset_num_proc: int | None = Nonenum_mini_batches: int = 1total_episodes: int | None = Nonelocal_rollout_forward_batch_size: int = 64num_sample_generations: int = 10response_length: int = 53stop_token: typing.Optional[typing.Literal['eos']] = Nonestop_token_id: int | None = Nonetemperature: float = 0.7missing_eos_penalty: float | None = Nonesft_model_path: str = 'EleutherAI/pythia-160m'world_size: int | None = Nonenum_total_batches: int | None = Nonemicro_batch_size: int | None = Nonelocal_batch_size: int | None = Nonebatch_size: int | None = Nonelocal_mini_batch_size: int | None = Nonemini_batch_size: int | None = Noneexp_name: str = 'ppo_config'reward_model_path: str = 'EleutherAI/pythia-160m'model_adapter_name: str | None = Noneref_adapter_name: str | None = Nonenum_ppo_epochs: int = 4whiten_rewards: bool = Falsekl_coef: float = 0.05kl_estimator: typing.Literal['k1', 'k3'] = 'k1'cliprange: float = 0.2vf_coef: float = 0.1cliprange_value: float = 0.2gamma: float = 1.0lam: float = 0.95ds3_gather_for_generation: bool = True )