class trl.SFTConfig
( output_dir: typing.Optional[str] = Noneoverwrite_output_dir: bool = Falsedo_train: bool = Falsedo_eval: bool = Falsedo_predict: bool = Falseeval_strategy: typing.Union[transformers.trainer_utils.IntervalStrategy, str] = 'no'prediction_loss_only: bool = Falseper_device_train_batch_size: int = 8per_device_eval_batch_size: int = 8per_gpu_train_batch_size: typing.Optional[int] = Noneper_gpu_eval_batch_size: typing.Optional[int] = Nonegradient_accumulation_steps: int = 1eval_accumulation_steps: typing.Optional[int] = Noneeval_delay: float = 0torch_empty_cache_steps: typing.Optional[int] = Nonelearning_rate: float = 2e-05weight_decay: float = 0.0adam_beta1: float = 0.9adam_beta2: float = 0.999adam_epsilon: float = 1e-08max_grad_norm: float = 1.0num_train_epochs: float = 3.0max_steps: int = -1lr_scheduler_type: typing.Union[transformers.trainer_utils.SchedulerType, str] = 'linear'lr_scheduler_kwargs: dict | str | None = Nonewarmup_ratio: float = 0.0warmup_steps: int = 0log_level: str = 'passive'log_level_replica: str = 'warning'log_on_each_node: bool = Truelogging_dir: typing.Optional[str] = Nonelogging_strategy: typing.Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps'logging_first_step: bool = Falselogging_steps: float = 10logging_nan_inf_filter: bool = Truesave_strategy: typing.Union[transformers.trainer_utils.SaveStrategy, str] = 'steps'save_steps: float = 500save_total_limit: typing.Optional[int] = Nonesave_safetensors: bool = Truesave_on_each_node: bool = Falsesave_only_model: bool = Falserestore_callback_states_from_checkpoint: bool = Falseno_cuda: bool = Falseuse_cpu: bool = Falseuse_mps_device: bool = Falseseed: int = 42data_seed: typing.Optional[int] = Nonejit_mode_eval: bool = Falsebf16: bool | None = Nonefp16: bool = Falsefp16_opt_level: str = 'O1'half_precision_backend: str = 'auto'bf16_full_eval: bool = Falsefp16_full_eval: bool = Falsetf32: typing.Optional[bool] = Nonelocal_rank: int = -1ddp_backend: typing.Optional[str] = Nonetpu_num_cores: typing.Optional[int] = Nonetpu_metrics_debug: bool = Falsedebug: typing.Union[str, list[transformers.debug_utils.DebugOption]] = ''dataloader_drop_last: bool = Falseeval_steps: typing.Optional[float] = Nonedataloader_num_workers: int = 0dataloader_prefetch_factor: typing.Optional[int] = Nonepast_index: int = -1run_name: typing.Optional[str] = Nonedisable_tqdm: typing.Optional[bool] = Noneremove_unused_columns: bool = Truelabel_names: typing.Optional[list[str]] = Noneload_best_model_at_end: bool = Falsemetric_for_best_model: typing.Optional[str] = Nonegreater_is_better: typing.Optional[bool] = Noneignore_data_skip: bool = Falsefsdp: typing.Union[list[transformers.trainer_utils.FSDPOption], str, NoneType] = Nonefsdp_min_num_params: int = 0fsdp_config: typing.Union[dict[str, typing.Any], str, NoneType] = Nonefsdp_transformer_layer_cls_to_wrap: typing.Optional[str] = Noneaccelerator_config: typing.Union[dict, str, NoneType] = Noneparallelism_config: typing.Optional[accelerate.parallelism_config.ParallelismConfig] = Nonedeepspeed: typing.Union[dict, str, NoneType] = Nonelabel_smoothing_factor: float = 0.0optim: typing.Union[transformers.training_args.OptimizerNames, str] = 'adamw_torch_fused'optim_args: typing.Optional[str] = Noneadafactor: bool = Falsegroup_by_length: bool = Falselength_column_name: str = 'length'report_to: typing.Union[NoneType, str, list[str]] = Noneproject: str = 'huggingface'trackio_space_id: typing.Optional[str] = 'trackio'ddp_find_unused_parameters: typing.Optional[bool] = Noneddp_bucket_cap_mb: typing.Optional[int] = Noneddp_broadcast_buffers: typing.Optional[bool] = Nonedataloader_pin_memory: bool = Truedataloader_persistent_workers: bool = Falseskip_memory_metrics: bool = Trueuse_legacy_prediction_loop: bool = Falsepush_to_hub: bool = Falseresume_from_checkpoint: typing.Optional[str] = Nonehub_model_id: typing.Optional[str] = Nonehub_strategy: typing.Union[transformers.trainer_utils.HubStrategy, str] = 'every_save'hub_token: typing.Optional[str] = Nonehub_private_repo: typing.Optional[bool] = Nonehub_always_push: bool = Falsehub_revision: typing.Optional[str] = Nonegradient_checkpointing: bool = Truegradient_checkpointing_kwargs: typing.Union[dict[str, typing.Any], str, NoneType] = Noneinclude_inputs_for_metrics: bool = Falseinclude_for_metrics: list = <factory>eval_do_concat_batches: bool = Truefp16_backend: str = 'auto'push_to_hub_model_id: typing.Optional[str] = Nonepush_to_hub_organization: typing.Optional[str] = Nonepush_to_hub_token: typing.Optional[str] = Nonemp_parameters: str = ''auto_find_batch_size: bool = Falsefull_determinism: bool = Falsetorchdynamo: typing.Optional[str] = Noneray_scope: typing.Optional[str] = 'last'ddp_timeout: int = 1800torch_compile: bool = Falsetorch_compile_backend: typing.Optional[str] = Nonetorch_compile_mode: typing.Optional[str] = Noneinclude_tokens_per_second: bool = Falseinclude_num_input_tokens_seen: typing.Union[str, bool] = Falseneftune_noise_alpha: typing.Optional[float] = Noneoptim_target_modules: typing.Union[NoneType, str, list[str]] = Nonebatch_eval_metrics: bool = Falseeval_on_start: bool = Falseuse_liger_kernel: bool = Falseliger_kernel_config: typing.Optional[dict[str, bool]] = Noneeval_use_gather_object: bool = Falseaverage_tokens_across_devices: bool = Truemodel_init_kwargs: dict[str, typing.Any] | None = Nonechat_template_path: str | None = Nonedataset_text_field: str = 'text'dataset_kwargs: dict[str, typing.Any] | None = Nonedataset_num_proc: int | None = Noneeos_token: str | None = Nonepad_token: str | None = Nonemax_length: int | None = 1024shuffle_dataset: bool = Falsepacking: bool = Falsepacking_strategy: str = 'bfd'padding_free: bool = Falsepad_to_multiple_of: int | None = Noneeval_packing: bool | None = Nonecompletion_only_loss: bool | None = Noneassistant_only_loss: bool = Falseloss_type: str = 'nll'activation_offloading: bool = False )
Parameters that control the model

model_init_kwargs (dict[str, Any], optional) — Keyword arguments for from_pretrained, used when the model argument of the SFTTrainer is provided as a string. If you’re training a MoE architecture and want to include the load balancing/auxilliary loss as a part of the final loss, remember to set output_router_logits=True in this dictionary.
chat_template_path (str, optional) — If specified, sets the model’s chat template. This can either be the path to a tokenizer (local directory or Hugging Face Hub model) or a direct path to a Jinja template file. When using a Jinja file, you must ensure that any special tokens referenced in the template are added to the tokenizer and that the model’s embedding layer is resized accordingly.
Parameters that control the data preprocessing

dataset_text_field (str, optional, defaults to "text") — Name of the column that contains text data in the dataset.
dataset_kwargs (dict[str, Any], optional) — Dictionary of optional keyword arguments for the dataset preparation. The only supported key is skip_prepare_dataset. When the model is a VLM, skip_prepare_dataset is automatically treated as True regardless of the provided value, since preprocessing is done on the fly.
dataset_num_proc (int, optional) — Number of processes to use for processing the dataset.
eos_token (str, optional) — Token used to indicate the end of a turn or sequence. If None, it defaults to processing_class.eos_token.
pad_token (str, optional) — Token used for padding. If None, it defaults to processing_class.pad_token, or if that is also None, it falls back to processing_class.eos_token.
max_length (int or None, optional, defaults to 1024) — Maximum length of the tokenized sequence. Sequences longer than max_length are truncated from the right. If None, no truncation is applied. When packing is enabled, this value sets the sequence length.
shuffle_dataset (bool, optional, defaults to False) — Whether to shuffle the dataset.
packing (bool, optional, defaults to False) — Whether to group multiple sequences into fixed-length blocks to improve computational efficiency and reduce padding. Uses max_length to define sequence length.
packing_strategy (str, optional, defaults to "bfd") — Strategy for packing sequences. Can be either "bfd" (best-fit decreasing, default), or "wrapped".
padding_free (bool, optional, defaults to False) — Whether to perform forward passes without padding by flattening all sequences in the batch into a single continuous sequence. This reduces memory usage by eliminating padding overhead. Currently, this is only supported with the FlashAttention 2 or 3, which can efficiently handle the flattened batch structure. When packing is enabled with strategy "bfd", padding-free is enabled, regardless of the value of this parameter.
pad_to_multiple_of (int, optional) — If set, the sequences will be padded to a multiple of this value.
eval_packing (bool, optional) — Whether to pack the eval dataset. If None, uses the same value as packing.
Parameters that control the training

completion_only_loss (bool, optional) — Whether to compute loss only on the completion part of the sequence. If set to True, loss is computed only on the completion, which is supported only for prompt-completion datasets. If False, loss is computed on the entire sequence. If None (default), the behavior depends on the dataset: loss is computed on the completion for prompt-completion datasets, and on the full sequence for language modeling datasets.
assistant_only_loss (bool, optional, defaults to False) — Whether to compute loss only on the assistant part of the sequence. If set to True, loss is computed only on the assistant responses, which is supported only for conversational datasets. If False, loss is computed on the entire sequence.
loss_type (str, optional, defaults to "nll") — Type of loss to use. Possible values are "nll" (negative log-likelihood, default) and "dft" (Dynamic Fine-Tuning, as described in this paper).
activation_offloading (bool, optional, defaults to False) — Whether to offload the activations to the CPU.
Configuration class for the SFTTrainer.

This class includes only the parameters that are specific to SFT training. For a full list of training arguments, please refer to the TrainingArguments documentation. Note that default values in this class may differ from those in TrainingArguments.

Using HfArgumentParser we can turn this class into argparse arguments that can be specified on the command line.