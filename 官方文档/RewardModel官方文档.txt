Reward Modeling
Overview
TRL supports the Outcome-supervised Reward Modeling (ORM) Trainer for training reward models.

This post-training method was contributed by Younes Belkada.

Quick start
This example demonstrates how to train a reward model using the RewardTrainer from TRL. We train a Qwen 3 0.6B model on the UltraFeedback dataset, large-scale, fine-grained, diverse preference dataset.
```python
from trl import RewardTrainer
from datasets import load_dataset

trainer = RewardTrainer(
    model="Qwen/Qwen3-0.6B",
    train_dataset=load_dataset("trl-lib/ultrafeedback_binarized", split="train"),
)
trainer.train()
```
Expected dataset type and format
RewardTrainer supports preference datasets type (both implicit and explicit prompt). The RewardTrainer is compatible with both standard and conversational dataset formats. When provided with a conversational dataset, the trainer will automatically apply the chat template to the dataset.
```python
# Standard preference (implicit prompt)
{"chosen": "The sky is blue.",
 "rejected": "The sky is green."}

# Conversational preference (implicit prompt)
{"chosen": [{"role": "user", "content": "What color is the sky?"},
            {"role": "assistant", "content": "It is blue."}],
 "rejected": [{"role": "user", "content": "What color is the sky?"},
              {"role": "assistant", "content": "It is green."}]}

# Standard preference (explicit prompt)
{"prompt": "The sky is",
 "chosen": " blue.",
 "rejected": " green."}

# Conversational preference (explicit prompt)
{"prompt": [{"role": "user", "content": "What color is the sky?"}],
 "chosen": [{"role": "assistant", "content": "It is blue."}],
 "rejected": [{"role": "assistant", "content": "It is green."}]}
```

If your dataset is not in one of these formats, you can preprocess it to convert it into the expected format. Here is an example with the lmarena-ai/arena-human-preference-55k dataset:

```python
from datasets import load_dataset
import json

dataset = load_dataset("lmarena-ai/arena-human-preference-55k")

# Filter out ties
dataset = dataset.filter(lambda example: example["winner_tie"] == 0)

# Create 'chosen' and 'rejected' fields based on the winner column
def response_a_b_to_chosen_rejected(example):
    if example["winner_model_a"] == 1:
        example["chosen"] = example["response_a"]
        example["rejected"] = example["response_b"]
    else:
        example["chosen"] = example["response_b"]
        example["rejected"] = example["response_a"]
    return example

dataset = dataset.map(response_a_b_to_chosen_rejected)

# Convert to conversational format
def make_conversation(example):
    prompt = json.loads(example["prompt"])[0]  # '["What color is the sky?"]' -> "What color is the sky?"
    chosen = json.loads(example["chosen"])[0]
    rejected = json.loads(example["rejected"])[0]
    return {
        "chosen": [{"role": "user", "content": prompt}, {"role": "assistant", "content": chosen}],
        "rejected": [{"role": "user", "content": prompt}, {"role": "assistant", "content": rejected}],
    }


dataset = dataset.map(make_conversation)

# Keep only necessary columns
dataset = dataset.select_columns(["chosen", "rejected"])

print(next(iter(dataset["train"])))
```
```python
{
    "chosen": [
        {"role": "user", "content": "Is it morally right to try to have a certain percentage of females on managerial positions?"},
        {"role": "assistant", "content": "The question of whether it is morally right to aim for a certain percentage of females..."},
    ],
    "rejected": [
        {"role": "user", "content": "Is it morally right to try to have a certain percentage of females on managerial positions?"},
        {"role": "assistant", "content": "As an AI, I don't have personal beliefs or opinions. However, ..."},
    ],
}
```
Looking deeper into the training method
Reward Models (RMs) are typically trained using supervised learning on datasets containing pairs of preferred and non-preferred responses. The goal is to learn a function that assigns higher scores to preferred responses, enabling the model to rank outputs based on preferences.

This section breaks down how reward modeling works in practice, covering the key steps: preprocessing and loss computation.

Preprocessing and tokenization
During training, each example is expected to contain a chosen and rejected field. For more details on the expected formats, see Dataset formats - Preference. The RewardTrainer tokenizes each input using the model‚Äôs tokenizer. If prompts and completions (chosen and rejected) are provided separately (explicit prompt case), they are concatenated before tokenization.

Computing the loss
The Bradley-Terry model is underdetermined, meaning that adding a constant to all rewards does not change the preference probabilities. To address this, Helping or Herding? Reward Model Ensembles Mitigate but do not Eliminate Reward Hacking proposes adding an auxiliary loss term that encourages the rewards to be centered around zero. This is controlled by the center_rewards_coefficient parameter in the RewardConfig. The recommended value is 1e-2.
Logged metrics
While training and evaluating we record the following reward metrics:

global_step: The total number of optimizer steps taken so far.
epoch: The current epoch number, based on dataset iteration.
num_tokens: The total number of tokens processed so far.
loss: The average loss over the last logging interval.
accuracy: The proportion of correct predictions (i.e., the model assigned a higher score to the chosen response than to the rejected one) averaged over the last logging interval.
min_reward: The minimum reward score assigned by the model. This value is averaged over the logging interval.
mean_reward: The average reward score assigned by the model over the last logging interval.
max_reward: The maximum reward score assigned by the model. This value is averaged over the logging interval.
margin: The average margin (difference between chosen and rejected rewards) over the last logging interval.
learning_rate: The current learning rate, which may change dynamically if a scheduler is used.
grad_norm: The L2 norm of the gradients, computed before gradient clipping.

Customization
Model initialization
You can directly pass the kwargs of the from_pretrained() method to the RewardConfig. For example, if you want to load a model in a different precision, analogous to
```python
model = AutoModelForSequenceClassification.from_pretrained("Qwen/Qwen3-0.6B", dtype=torch.bfloat16)
```
you can do so by passing the model_init_kwargs={"dtype": torch.bfloat16} argument to the RewardConfig.
```python
from trl import RewardConfig

training_args = RewardConfig(
    model_init_kwargs={"dtype": torch.bfloat16},
)
```

Note that all keyword arguments of from_pretrained() are supported, except for num_labels, which is automatically set to 1.
Train adapters with PEFT
We support tight integration with ü§ó PEFT library, allowing any user to conveniently train adapters and share them on the Hub, rather than training the entire model.
```python
from datasets import load_dataset
from trl import RewardTrainer
from peft import LoraConfig

dataset = load_dataset("trl-lib/ultrafeedback_binarized", split="train")

trainer = RewardTrainer(
    "Qwen/Qwen3-4B",
    train_dataset=dataset,
    peft_config=LoraConfig(modules_to_save=["score"])  # important to include the score head when base model is not a sequence classification model
)

trainer.train()
```
You can also continue training your PeftModel. For that, first load a PeftModel outside RewardTrainer and pass it directly to the trainer without the peft_config argument being passed.
```python
from datasets import load_dataset
from trl import RewardTrainer
from peft import AutoPeftModelForCausalLM

model = AutoPeftModelForCausalLM.from_pretrained("trl-lib/Qwen3-4B-Reward-LoRA", is_trainable=True)
dataset = load_dataset("trl-lib/Capybara", split="train")

trainer = RewardTrainer(
    model=model,
    train_dataset=dataset,
)

trainer.train()
```
When training adapters, you typically use a higher learning rate (‚âà1e‚Äë3) since only new parameters are being learned.
```python
RewardConfig(learning_rate=1e-3, ...)
```
Tool Calling with Reward Modeling
The RewardTrainer fully supports fine-tuning models with tool calling capabilities. In this case, each dataset example should include:

The conversation messages, including any tool calls (tool_calls) and tool responses (tool role messages)
The list of available tools in the tools column, typically provided as JSON schemas
For details on the expected dataset structure, see the Dataset Format ‚Äî Tool Calling section.

RewardTrainer
class trl.RewardTrainer
( model: str | transformers.modeling_utils.PreTrainedModelargs: trl.trainer.reward_config.RewardConfig | None = Nonedata_collator: typing.Optional[typing.Callable[[list[typing.Any]], dict[str, typing.Any]]] = Nonetrain_dataset: datasets.arrow_dataset.Dataset | datasets.iterable_dataset.IterableDataset | None = Noneeval_dataset: datasets.arrow_dataset.Dataset | dict[str, datasets.arrow_dataset.Dataset] | None = Noneprocessing_class: transformers.tokenization_utils_base.PreTrainedTokenizerBase | None = Nonecompute_metrics: collections.abc.Callable[[transformers.trainer_utils.EvalPrediction], dict] | None = Nonecallbacks: list[transformers.trainer_callback.TrainerCallback] | None = Noneoptimizers: tuple = (None, None)optimizer_cls_and_kwargs: tuple[type[torch.optim.optimizer.Optimizer], dict[str, typing.Any]] | None = Nonepreprocess_logits_for_metrics: collections.abc.Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None = Nonepeft_config: PeftConfig | None = None )
Parameters

model (str | PreTrainedModel) ‚Äî Model to be trained. Can be either:
A string, being the model id of a pretrained model hosted inside a model repo on huggingface.co, or a path to a directory containing model weights saved using save_pretrained, e.g., './my_model_directory/'. The model is loaded using AutoModelForSequenceClassification.from_pretrained with the keyword arguments in args.model_init_kwargs.
A sequence classification PreTrainedModel object.
args (RewardConfig, optional) ‚Äî Configuration for this trainer. If None, a default configuration is used.
data_collator (DataCollator, optional) ‚Äî Function to use to form a batch from a list of elements of the processed train_dataset or eval_dataset. Will default to DataCollatorForPreference.
train_dataset (Dataset or IterableDataset) ‚Äî Dataset to use for training. This trainer supports preference type (both implicit and explicit prompt). The format of the samples can be either:
Standard: Each sample contains plain text.
Conversational: Each sample contains structured messages (e.g., role and content).
The trainer also supports processed datasets (tokenized) as long as they contain an chosen_input_ids and rejected_input_ids fields.

eval_dataset (Dataset, IterableDataset or dict[str, Dataset | IterableDataset]) ‚Äî Dataset to use for evaluation. It must meet the same requirements as train_dataset.
processing_class (PreTrainedTokenizerBase, optional) ‚Äî Tokenizer used to process the data. If None, the tokenizer is loaded from the model‚Äôs name with from_pretrained. A padding token, processing_class.pad_token, must be set. If the processing class has not set a padding token, processing_class.eos_token will be used as the default.
compute_metrics (Callable[[EvalPrediction], dict], optional) ‚Äî The function that will be used to compute metrics at evaluation. Must take a EvalPrediction and return a dictionary string to metric values. When passing RewardConfig with batch_eval_metrics set to True, your compute_metrics function must take a boolean compute_result argument. This will be triggered after the last eval batch to signal that the function needs to calculate and return the global summary statistics rather than accumulating the batch-level statistics.
callbacks (list of TrainerCallback, optional) ‚Äî List of callbacks to customize the training loop. Will add those to the list of default callbacks detailed in here.
If you want to remove one of the default callbacks used, use the remove_callback method.

optimizers (tuple[torch.optim.Optimizer | None, torch.optim.lr_scheduler.LambdaLR | None], optional, defaults to (None, None)) ‚Äî A tuple containing the optimizer and the scheduler to use. Will default to an instance of AdamW on your model and a scheduler given by get_linear_schedule_with_warmup controlled by args.
optimizer_cls_and_kwargs (tuple[Type[torch.optim.Optimizer], Dict[str, Any]], optional) ‚Äî A tuple containing the optimizer class and keyword arguments to use. Overrides optim and optim_args in args. Incompatible with the optimizers argument.
Unlike optimizers, this argument avoids the need to place model parameters on the correct devices before initializing the Trainer.

preprocess_logits_for_metrics (Callable[[torch.Tensor, torch.Tensor], torch.Tensor], optional) ‚Äî A function that preprocess the logits right before caching them at each evaluation step. Must take two tensors, the logits and the labels, and return the logits once processed as desired. The modifications made by this function will be reflected in the predictions received by compute_metrics.
Note that the labels (second parameter) will be None if the dataset does not have them.

peft_config (PeftConfig, optional) ‚Äî PEFT configuration used to wrap the model. If None, the model is not wrapped. Note that if the loaded model is a causal LM, it‚Äôs highly recommended to set modules_to_save=["score"] in the PEFT configuration to ensure that the reward head is properly trained.
Trainer for Outcome-supervised Reward Models (ORM).

This class is a wrapper around the Trainer class and inherits all of its attributes and methods.

Example:
```python
from trl import RewardTrainer
from datasets import load_dataset

dataset = load_dataset("trl-lib/ultrafeedback_binarized", split="train")

trainer = RewardTrainer(model="Qwen/Qwen2.5-0.5B-Instruct", train_dataset=dataset)
trainer.train()
```
train
( resume_from_checkpoint: typing.Union[str, bool, NoneType] = Nonetrial: typing.Union[ForwardRef('optuna.Trial'), dict[str, typing.Any], NoneType] = Noneignore_keys_for_eval: typing.Optional[list[str]] = None**kwargs: typing.Any )
Parameters

resume_from_checkpoint (str or bool, optional) ‚Äî If a str, local path to a saved checkpoint as saved by a previous instance of Trainer. If a bool and equals True, load the last checkpoint in args.output_dir as saved by a previous instance of Trainer. If present, training will resume from the model/optimizer/scheduler states loaded here.
trial (optuna.Trial or dict[str, Any], optional) ‚Äî The trial run or the hyperparameter dictionary for hyperparameter search.
ignore_keys_for_eval (list[str], optional) ‚Äî A list of keys in the output of your model (if it is a dictionary) that should be ignored when gathering predictions for evaluation during the training.
kwargs (dict[str, Any], optional) ‚Äî Additional keyword arguments used to hide deprecated arguments
Main training entry point.
save_model
( output_dir: typing.Optional[str] = None_internal_call: bool = False )

Will save the model, so you can reload it using from_pretrained().

Will only save from the main process.
push_to_hub
( commit_message: typing.Optional[str] = 'End of training'blocking: bool = Truetoken: typing.Optional[str] = Nonerevision: typing.Optional[str] = None**kwargs )

Parameters

commit_message (str, optional, defaults to "End of training") ‚Äî Message to commit while pushing.
blocking (bool, optional, defaults to True) ‚Äî Whether the function should return only when the git push has finished.
token (str, optional, defaults to None) ‚Äî Token with write permission to overwrite Trainer‚Äôs original args.
revision (str, optional) ‚Äî The git revision to commit from. Defaults to the head of the ‚Äúmain‚Äù branch.
kwargs (dict[str, Any], optional) ‚Äî Additional keyword arguments passed along to ~Trainer.create_model_card.
Upload self.model and self.processing_class to the ü§ó model hub on the repo self.args.hub_model_id.

RewardConfig
class trl.RewardConfig
( output_dir: typing.Optional[str] = Noneoverwrite_output_dir: bool = Falsedo_train: bool = Falsedo_eval: bool = Falsedo_predict: bool = Falseeval_strategy: typing.Union[transformers.trainer_utils.IntervalStrategy, str] = 'no'prediction_loss_only: bool = Falseper_device_train_batch_size: int = 8per_device_eval_batch_size: int = 8per_gpu_train_batch_size: typing.Optional[int] = Noneper_gpu_eval_batch_size: typing.Optional[int] = Nonegradient_accumulation_steps: int = 1eval_accumulation_steps: typing.Optional[int] = Noneeval_delay: float = 0torch_empty_cache_steps: typing.Optional[int] = Nonelearning_rate: float = 0.0001weight_decay: float = 0.0adam_beta1: float = 0.9adam_beta2: float = 0.999adam_epsilon: float = 1e-08max_grad_norm: float = 1.0num_train_epochs: float = 3.0max_steps: int = -1lr_scheduler_type: typing.Union[transformers.trainer_utils.SchedulerType, str] = 'linear'lr_scheduler_kwargs: dict | str | None = Nonewarmup_ratio: float = 0.0warmup_steps: int = 0log_level: str = 'passive'log_level_replica: str = 'warning'log_on_each_node: bool = Truelogging_dir: typing.Optional[str] = Nonelogging_strategy: typing.Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps'logging_first_step: bool = Falselogging_steps: float = 10logging_nan_inf_filter: bool = Truesave_strategy: typing.Union[transformers.trainer_utils.SaveStrategy, str] = 'steps'save_steps: float = 500save_total_limit: typing.Optional[int] = Nonesave_safetensors: bool = Truesave_on_each_node: bool = Falsesave_only_model: bool = Falserestore_callback_states_from_checkpoint: bool = Falseno_cuda: bool = Falseuse_cpu: bool = Falseuse_mps_device: bool = Falseseed: int = 42data_seed: typing.Optional[int] = Nonejit_mode_eval: bool = Falsebf16: bool | None = Nonefp16: bool = Falsefp16_opt_level: str = 'O1'half_precision_backend: str = 'auto'bf16_full_eval: bool = Falsefp16_full_eval: bool = Falsetf32: typing.Optional[bool] = Nonelocal_rank: int = -1ddp_backend: typing.Optional[str] = Nonetpu_num_cores: typing.Optional[int] = Nonetpu_metrics_debug: bool = Falsedebug: typing.Union[str, list[transformers.debug_utils.DebugOption]] = ''dataloader_drop_last: bool = Falseeval_steps: typing.Optional[float] = Nonedataloader_num_workers: int = 0dataloader_prefetch_factor: typing.Optional[int] = Nonepast_index: int = -1run_name: typing.Optional[str] = Nonedisable_tqdm: typing.Optional[bool] = Noneremove_unused_columns: bool = Truelabel_names: typing.Optional[list[str]] = Noneload_best_model_at_end: bool = Falsemetric_for_best_model: typing.Optional[str] = Nonegreater_is_better: typing.Optional[bool] = Noneignore_data_skip: bool = Falsefsdp: typing.Union[list[transformers.trainer_utils.FSDPOption], str, NoneType] = Nonefsdp_min_num_params: int = 0fsdp_config: typing.Union[dict[str, typing.Any], str, NoneType] = Nonefsdp_transformer_layer_cls_to_wrap: typing.Optional[str] = Noneaccelerator_config: typing.Union[dict, str, NoneType] = Noneparallelism_config: typing.Optional[accelerate.parallelism_config.ParallelismConfig] = Nonedeepspeed: typing.Union[dict, str, NoneType] = Nonelabel_smoothing_factor: float = 0.0optim: typing.Union[transformers.training_args.OptimizerNames, str] = 'adamw_torch_fused'optim_args: typing.Optional[str] = Noneadafactor: bool = Falsegroup_by_length: bool = Falselength_column_name: str = 'length'report_to: typing.Union[NoneType, str, list[str]] = Noneproject: str = 'huggingface'trackio_space_id: typing.Optional[str] = 'trackio'ddp_find_unused_parameters: typing.Optional[bool] = Noneddp_bucket_cap_mb: typing.Optional[int] = Noneddp_broadcast_buffers: typing.Optional[bool] = Nonedataloader_pin_memory: bool = Truedataloader_persistent_workers: bool = Falseskip_memory_metrics: bool = Trueuse_legacy_prediction_loop: bool = Falsepush_to_hub: bool = Falseresume_from_checkpoint: typing.Optional[str] = Nonehub_model_id: typing.Optional[str] = Nonehub_strategy: typing.Union[transformers.trainer_utils.HubStrategy, str] = 'every_save'hub_token: typing.Optional[str] = Nonehub_private_repo: typing.Optional[bool] = Nonehub_always_push: bool = Falsehub_revision: typing.Optional[str] = Nonegradient_checkpointing: bool = Truegradient_checkpointing_kwargs: typing.Union[dict[str, typing.Any], str, NoneType] = Noneinclude_inputs_for_metrics: bool = Falseinclude_for_metrics: list = <factory>eval_do_concat_batches: bool = Truefp16_backend: str = 'auto'push_to_hub_model_id: typing.Optional[str] = Nonepush_to_hub_organization: typing.Optional[str] = Nonepush_to_hub_token: typing.Optional[str] = Nonemp_parameters: str = ''auto_find_batch_size: bool = Falsefull_determinism: bool = Falsetorchdynamo: typing.Optional[str] = Noneray_scope: typing.Optional[str] = 'last'ddp_timeout: int = 1800torch_compile: bool = Falsetorch_compile_backend: typing.Optional[str] = Nonetorch_compile_mode: typing.Optional[str] = Noneinclude_tokens_per_second: bool = Falseinclude_num_input_tokens_seen: typing.Union[str, bool] = Falseneftune_noise_alpha: typing.Optional[float] = Noneoptim_target_modules: typing.Union[NoneType, str, list[str]] = Nonebatch_eval_metrics: bool = Falseeval_on_start: bool = Falseuse_liger_kernel: bool = Falseliger_kernel_config: typing.Optional[dict[str, bool]] = Noneeval_use_gather_object: bool = Falseaverage_tokens_across_devices: bool = Truemodel_init_kwargs: dict[str, typing.Any] | None = Nonechat_template_path: str | None = Nonedisable_dropout: bool = Truedataset_num_proc: int | None = Noneeos_token: str | None = Nonepad_token: str | None = Nonemax_length: int | None = 1024pad_to_multiple_of: int | None = Nonecenter_rewards_coefficient: float | None = Noneactivation_offloading: bool = False )
Parameters that control the model

model_init_kwargs (dict[str, Any], optional) ‚Äî Keyword arguments for from_pretrained, used when the model argument of the RewardTrainer is provided as a string. If you‚Äôre training a MoE architecture and want to include the load balancing/auxilliary loss as a part of the final loss, remember to set output_router_logits=True in this dictionary.
chat_template_path (str, optional) ‚Äî If specified, sets the model‚Äôs chat template. This can either be the path to a tokenizer (local directory or Hugging Face Hub model) or a direct path to a Jinja template file. When using a Jinja file, you must ensure that any special tokens referenced in the template are added to the tokenizer and that the model‚Äôs embedding layer is resized accordingly.
disable_dropout (bool, optional, defaults to True) ‚Äî Whether to disable dropout in the model.
Parameters that control the data preprocessing

dataset_num_proc (int, optional) ‚Äî Number of processes to use for processing the dataset.
eos_token (str, optional) ‚Äî Token used to indicate the end of a turn or sequence. If None, it defaults to processing_class.eos_token.
pad_token (str, optional) ‚Äî Token used for padding. If None, it defaults to processing_class.pad_token, or if that is also None, it falls back to processing_class.eos_token.
max_length (int or None, optional, defaults to 1024) ‚Äî Maximum length of the tokenized sequence. Samples are filtered out if either chosen or rejected sequence exceeds this value. If None, no filtering is applied.
pad_to_multiple_of (int, optional) ‚Äî If set, the sequences will be padded to a multiple of this value.
Parameters that control the training

center_rewards_coefficient (float, optional) ‚Äî Coefficient to incentivize the reward model to output mean-zero rewards (proposed by https://huggingface.co/papers/2312.09244, Eq. 2). Recommended value: 0.01.
activation_offloading (bool, optional, defaults to False) ‚Äî Whether to offload the activations to the CPU.
Configuration class for the RewardTrainer.

This class includes only the parameters that are specific to Reward training. For a full list of training arguments, please refer to the TrainingArguments documentation. Note that default values in this class may differ from those in TrainingArguments.

Using HfArgumentParser we can turn this class into argparse arguments that can be specified on the command line.

DataCollatoForPreference
class trl.trainer.reward_trainer.DataCollatorForPreference
( pad_token_id: intpad_to_multiple_of: int | None = Nonereturn_tensors: str = 'pt' )

Parameters

pad_token_id (int) ‚Äî Token ID to use for padding.
pad_to_multiple_of (int, optional) ‚Äî If set, the sequences will be padded to a multiple of this value.
return_tensors (str, optional, defaults to "pt") ‚Äî Type of Tensor to return. Only "pt" is currently supported.
Data collator used for preference data. Inputs are dynamically padded to the maximum length of a batch.

This collator expects each example in the input list to be a dictionary containing the "chosen_input_ids" and "rejected_input_ids" keys. The collator returns a dictionary containing the following keys:

"input_ids": Tensor of input IDs, padded to the maximum length of the batch. The first half of the batch corresponds to the "chosen_input_ids" and the second half to the "rejected_input_ids".
"attention_mask": Tensor of attention mask, padded to the maximum length of the batch.
Optionally, the examples can contain a "margin" key, in which case the returned dictionary will also contain a "margin" key with a tensor of margins.

Examples:
```python
from trl.trainer.reward_trainer import DataCollatorForPreference

collator = DataCollatorForPreference(pad_token_id=0)
examples = [
    {"chosen_input_ids": [1, 2, 3], "rejected_input_ids": [4, 5]},
    {"chosen_input_ids": [6, 7], "rejected_input_ids": [8]},
]
collator(examples)

examples = [
    {"chosen_input_ids": [1, 2, 3], "rejected_input_ids": [4, 5], "margin": 0.5},
    {"chosen_input_ids": [6, 7], "rejected_input_ids": [8], "margin": 0.0},
]
collator(examples)
```
